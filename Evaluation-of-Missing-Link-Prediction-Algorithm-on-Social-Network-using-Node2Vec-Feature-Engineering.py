# -*- coding: utf-8 -*-
"""Link Prediction MS Thesis SNA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-W-xS27mkZuGdianqN-zrtdmyNnqwX0j

This file belongs to Araf Toa Sanjeed Khan. Please contact araf.khan03@gmail.com for any questions. Thank you!

This file contains all the code works for the thesis submitted to the Department of Mathematical and Physical Sciences, East West University, in partial fulfilment of the requirement for the degree of Master of Science in Applied Statistics

Thesis Title: Evaluation of Different Missing Link Prediction Algorithms on Social Network Data using Node2Vec Feature Engineering

Submitted by: 
Araf Toa Sanjeed Khan, ID: 2019-2-82-014

Supervisor: 
Dr. Md. Sohel Rana, Associate professor , Department of Mathematical and Physical Sciences, East West University


Data Source: FB Pages Tv Shows from https://networkrepository.com/fb-pages-tvshow.php

@inproceedings{nr,
     title={The Network Data Repository with Interactive Graph Analytics and Visualization},
     author={Ryan A. Rossi and Nesreen K. Ahmed},
     booktitle={AAAI},
     url={https://networkrepository.com},
     year={2015}
}
"""

import pandas as pd
import numpy as np
import random
import networkx as nx
from tqdm import tqdm
import re
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/data/fb-pages-tvshow.edges"
df = pd.read_csv(path, names=["From", "Neighbour"])

df

# create graph
G = nx.from_pandas_edgelist(df, "From", "Neighbour", create_using=nx.Graph())

# plot graph
plt.figure(figsize=(10,10))

pos = nx.random_layout(G, seed=23)
nx.draw(G, with_labels=False,  pos = pos, node_size = 1, alpha = 0.5, width = 0.5)

plt.show()

G.number_of_edges()
G.number_of_nodes()
nx.number_connected_components(G)

G.number_of_nodes()

nx.number_connected_components(G)

# combine all nodes in a list
node_list = list(G)

# remove duplicate items from the list
node_list = list(dict.fromkeys(node_list))

# build adjacency matrix
adj_G = nx.to_numpy_matrix(G, nodelist = node_list)
adj_G.shape
adj_G

adj_G.shape

adj_G

all_unconnected_pairs_df_path = "/content/drive/MyDrive/data/all_unconnected_pairs_df.csv"
all_unconnected_pairs_df = pd.read_csv(all_unconnected_pairs_df_path)
all_unconnected_pairs_df = all_unconnected_pairs_df.iloc[:,-2:]
all_unconnected_pairs = all_unconnected_pairs_df.values.tolist()

# get unconnected node-pairs
 all_unconnected_pairs = []

# traverse adjacency matrix
offset = 0
for i in tqdm(range(adj_G.shape[0])):
  for j in range(offset,adj_G.shape[1]):
    if i != j:
      if nx.shortest_path_length(G, i, j) <=2:
        if adj_G[i,j] == 0:
          all_unconnected_pairs.append([node_list[i],node_list[j]])

  offset = offset + 1

all_unconnected_pairs

len(all_unconnected_pairs)

node_1_unlinked = [i[0] for i in all_unconnected_pairs]
node_2_unlinked = [i[1] for i in all_unconnected_pairs]

data = pd.DataFrame({'node_1':node_1_unlinked, 
                     'node_2':node_2_unlinked})

# add target variable 'link'
data['link'] = 0
data

omissible_links_index_df_path = "/content/drive/MyDrive/data/omissible_links_index_df.csv"
omissible_links_index_df = pd.read_csv(omissible_links_index_df_path)
omissible_links_index_df = omissible_links_index_df.iloc[:,-1:]
omissible_links_index = omissible_links_index_df.values.tolist()
omissible_links_index = [item for sublist in omissible_links_index for item in sublist]
omissible_links_index

initial_node_count = len(G.nodes)

df_temp = df.copy()

# empty list to store removable links
omissible_links_index = []

for i in tqdm(df.index.values):
  
  # remove a node pair and build a new graph
  G_temp = nx.from_pandas_edgelist(df_temp.drop(index = i), "From", "Neighbour", create_using=nx.Graph())
  
  # check there is no spliting of graph and number of nodes is same
  if (nx.number_connected_components(G_temp) == 1) and (len(G_temp.nodes) == initial_node_count):
    omissible_links_index.append(i)
    df_temp = df_temp.drop(index = i)

len(omissible_links_index)

# create dataframe of removable edges
df_ghost = df.loc[omissible_links_index]

# add the target variable 'link'
df_ghost['link'] = 1
df_ghost = df_ghost.rename(columns={'From': 'node_1', 'Neighbour': 'node_2'})
df_ghost

#df_ghost.index.values

data = data.append(df_ghost[['node_1', 'node_2', 'link']], ignore_index=True)
data

data['link'].value_counts()

# drop removable edges
df_partial = df.drop(index=df_ghost.index.values)

# build graph
G_data = nx.from_pandas_edgelist(df_partial, "From", "Neighbour", create_using=nx.Graph())

df_partial

!pip install node2vec

from node2vec import Node2Vec

# Generate walks
node2vec = Node2Vec(G_data, dimensions=100, walk_length=16, num_walks=50)

# train node2vec model
n2w_model = node2vec.fit(window=7, min_count=1)

x = [(n2w_model[str(i)]+n2w_model[str(j)]) for i,j in zip(data['node_1'], data['node_2'])]

x

xtrain, xtest, ytrain, ytest = train_test_split(np.array(x), data['link'], 
                                                test_size = 0.4, 
                                                random_state = 35)

lr = LogisticRegression(class_weight="balanced")
lr.fit(xtrain, ytrain)

predictions = lr.predict_proba(xtest)

predictions[:1]

roc_auc_score(ytest, predictions[:,1])

from sklearn.metrics import accuracy_score

logreg = LogisticRegression(class_weight="balanced")
logreg.fit(xtrain, ytrain) #This is where the training is taking place
y_pred_logreg = logreg.predict(xtest) #Making predictions to test the model on test data
print('Logistic Regression Train accuracy %s' % logreg.score(xtrain, ytrain)) #Train accuracy
#Logistic Regression Train accuracy 0.8333333333333334
print('Logistic Regression Test accuracy %s' % accuracy_score(y_pred_logreg, ytest)) #Test accuracy
#Logistic Regression Test accuracy 0.5
print(confusion_matrix(ytest, y_pred_logreg)) #Confusion matrix
print(classification_report(ytest, y_pred_logreg)) #Classification Report

import sklearn.metrics as metrics
# calculate the fpr and tpr for all thresholds of the classification
probs = lr.predict_proba(xtest)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(ytest, preds)
roc_auc = metrics.auc(fpr, tpr)

# method I: plt
import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic for Logistic Regression')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(max_depth=2, random_state=0)
clf.fit(xtrain, ytrain)

import sklearn.metrics as metrics
# calculate the fpr and tpr for all thresholds of the classification
probs = clf.predict_proba(xtest)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(ytest, preds)
roc_auc = metrics.auc(fpr, tpr)

# method I: plt
import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic for Random Forest Classifier')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(xtrain, ytrain)

import sklearn.metrics as metrics
# calculate the fpr and tpr for all thresholds of the classification
probs = gnb.predict_proba(xtest)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(ytest, preds)
roc_auc = metrics.auc(fpr, tpr)

# method I: plt
import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic for Gaussian Naive Bayes')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

import lightgbm as lgbm

train_data = lgbm.Dataset(xtrain, ytrain)
test_data = lgbm.Dataset(xtest, ytest)

# define parameters
parameters = {
    'objective': 'binary',
    'metric': 'auc',
    'is_unbalance': 'true',
    'feature_fraction': 0.5,
    'bagging_fraction': 0.5,
    'bagging_freq': 20,
    'num_threads' : 2,
    'seed' : 76
}

# train lightGBM model
model = lgbm.train(parameters,
                   train_data,
                   valid_sets=test_data,
                   num_boost_round=1000,
                   early_stopping_rounds=20)